{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Mini Project 2: Local Feature Matching](https://www.cc.gatech.edu/~hays/compvision/proj2/proj2.pdf)\n",
    "\n",
    "Esta notebook de iPython:  \n",
    "(1) Carga y redimensiona imágenes  \n",
    "(2) Encuentra puntos de interés en dichas imágenes                 (usted implementará esto)  \n",
    "(3) Describe cada punto de interés con un local feature            (usted implementará esto)  \n",
    "(4) Encuentra matching de features                                 (usted implementará esto)  \n",
    "(5) Visualiza dichos matches  \n",
    "(6) Evalua los matches basado en correpondencias de datos validados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from proj2_code.utils import load_image, PIL_resize, rgb2gray, normalize_img, verify\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "# Notre Dame\n",
    "image1 = load_image('../data/1a_notredame.jpg')\n",
    "image2 = load_image('../data/1b_notredame.jpg')\n",
    "eval_file = '../ground_truth/notredame.pkl'\n",
    "\n",
    "# # Mount Rushmore -- this pair is relatively easy (still harder than Notre Dame, though)\n",
    "# image1 = load_image('../data/2a_rushmore.jpg')\n",
    "# image2 = load_image('../data/2b_rushmore.jpg')\n",
    "# eval_file = '../ground_truth/rushmore.pkl'\n",
    "\n",
    "# # Episcopal Gaudi -- This pair is relatively difficult\n",
    "# image1 = load_image('../data/3a_gaudi.jpg')\n",
    "# image2 = load_image('../data/3b_gaudi.jpg')\n",
    "# eval_file = '../ground_truth/gaudi.pkl'\n",
    "\n",
    "scale_factor = 0.5\n",
    "image1 = PIL_resize(image1, (int(image1.shape[1]*scale_factor), int(image1.shape[0]*scale_factor)))\n",
    "image2 = PIL_resize(image2, (int(image2.shape[1]*scale_factor), int(image2.shape[0]*scale_factor)))\n",
    "\n",
    "image1_bw = rgb2gray(image1)\n",
    "image2_bw = rgb2gray(image2)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(image1)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(image2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Parte 1: Harris Corner Detector \n",
    "## Encuentra puntos distintivos en cada imagen (Szeliski 4.1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El detector de esquinas de Harris y SIFT dependen en gran medida de la información de gradiente de la imagen. Van a implementar `compute_image_gradients()` y luego visualizaremos la magnitud de las gracientes de las imágenes.  \n",
    "\n",
    "¿Qué áreas tienen las magnitudes mas altas y por qué?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_code.part1_harris_corner import compute_image_gradients\n",
    "from proj2_unit_tests.test_part1_harris_corner import test_compute_image_gradients\n",
    "\n",
    "print('compute_image_gradients(): ', verify(test_compute_image_gradients))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.axis('off')\n",
    "\n",
    "Ix, Iy = compute_image_gradients(image1_bw)\n",
    "gradient_magnitudes = np.sqrt(Ix**2 + Iy**2)\n",
    "gradient_magnitudes = normalize_img(gradient_magnitudes)\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(r'$\\sqrt{I_x^2 + I_y^2}$')\n",
    "plt.imshow( (gradient_magnitudes*255).astype(np.uint8))\n",
    "\n",
    "Ix, Iy = compute_image_gradients(image2_bw)\n",
    "gradient_magnitudes = np.sqrt(Ix**2 + Iy**2)\n",
    "gradient_magnitudes = normalize_img(gradient_magnitudes)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(r'$\\sqrt{I_x^2 + I_y^2}$')\n",
    "plt.imshow( (gradient_magnitudes*255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora calcularemos los elementos de la matriz M de segundo momento $s_x^2, s_y^2, s_x s_y$ en cada pixel, lo cual agrega información de las gradientes en vecindarios locales. Vamos a usar un filtro Gaussiano 2D para agregar información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_code.part1_harris_corner import second_moments\n",
    "\n",
    "from proj2_unit_tests.test_part1_harris_corner import (\n",
    "    test_get_gaussian_kernel_2D_pytorch_peak,\n",
    "    test_get_gaussian_kernel_2D_pytorch_sumsto1,\n",
    "    test_get_gaussian_kernel_2D_pytorch,\n",
    "    test_second_moments\n",
    ")\n",
    "\n",
    "print('get_gaussian_kernel_2D_pytorch_peak():', verify(test_get_gaussian_kernel_2D_pytorch_peak))\n",
    "print('get_gaussian_kernel_2D_pytorch_sumsto1():', verify(test_get_gaussian_kernel_2D_pytorch_sumsto1))\n",
    "print('get_gaussian_kernel_2D_pytorch():', verify(test_get_gaussian_kernel_2D_pytorch))\n",
    "print('second_moments():', verify(test_second_moments))\n",
    "\n",
    "sx2, sy2, sxsy = second_moments(image1_bw, ksize = 7, sigma = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si comparamos $s_x^2$, $s_y^2$, y $s_x s_y$ con $I_x$ y $I_y$, tenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_code.utils import normalize_img\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "Ix, Iy = compute_image_gradients(image1_bw)\n",
    "plt.subplot(2,3,1); plt.title(r'$I_x$')\n",
    "plt.imshow( (normalize_img(np.abs(Ix))*255).astype(np.uint8))\n",
    "plt.subplot(2,3,2); plt.title(r'$I_y$')\n",
    "plt.imshow( (normalize_img(np.abs(Iy))*255).astype(np.uint8))\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "plt.title(r'$s_x^2$')\n",
    "plt.imshow( (normalize_img(np.abs(sx2))*255).astype(np.uint8))\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "plt.title(r'$s_y^2$')\n",
    "plt.imshow( (normalize_img(np.abs(sy2))*255).astype(np.uint8))\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "plt.title(r'$s_xs_y$')\n",
    "plt.imshow( (normalize_img(np.abs(sxsy))*255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nótese que $s_xs_y$ es alto donde tenemos las gradientes fuertes en ambas direcciones x e y (esquinas por ejemplo y la rosa central).\n",
    "\n",
    "Ahora usaremos estos 'segundos momentos' para calcular el 'Score de Esquinas' R -- un mapa de respuesta a esquinas -- como una función de las imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_code.part1_harris_corner import compute_harris_response_map\n",
    "from proj2_unit_tests.test_part1_harris_corner import test_compute_harris_response_map\n",
    "\n",
    "print('compute_harris_response_map(): ', verify(test_compute_harris_response_map))\n",
    "\n",
    "R = compute_harris_response_map(image1_bw)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(image1_bw, cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(r'$R$')\n",
    "plt.imshow(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las áreas brillantes son áreas que tienen mas alta posibilidad de ser esquinas.\n",
    "\n",
    "AHora implementaremos non-max suppression para encontrar máximos locales en el mapa de respuestas o R de 2D.  Una manera simple de hacer non-maximum suppression es simplemente eligiendo un máximo local sobre una ventana de tamaño (u,v). Esto puede lograrse utilizando max-pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_code.part1_harris_corner import maxpool_numpy\n",
    "from proj2_unit_tests.test_part1_harris_corner import test_maxpool_numpy, test_nms_maxpool_pytorch\n",
    "from proj2_code.utils import verify\n",
    "\n",
    "print('maxpool_numpy(): ', verify(test_maxpool_numpy))\n",
    "\n",
    "toy_response_map = np.array(\n",
    "[\n",
    "    [1,2,2,1,2],\n",
    "    [1,6,2,1,1],\n",
    "    [2,2,1,1,1],\n",
    "    [1,1,1,7,1],\n",
    "    [1,1,1,1,1]\n",
    "]).astype(np.float32)\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(toy_response_map.astype(np.uint8))\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "maxpooled_image = maxpool_numpy(toy_response_map, ksize=3)\n",
    "plt.imshow(maxpooled_image.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada una grilla de 5x5 que contiene scores de respuestas R, non-max suppression nos va a permitir elegir valores que son máximos locales. Si utilizamos como ejemplo la grilla de arriba y solicitamos los $k=2$ puntajes de respuesta obtendriamos (1,1) y (3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_code.part1_harris_corner import nms_maxpool_pytorch\n",
    "\n",
    "print('nms_maxpool_pytorch(): ', verify(test_nms_maxpool_pytorch))\n",
    "\n",
    "x_coords, y_coords, confidences = nms_maxpool_pytorch(toy_response_map, k=2, ksize=3)\n",
    "print('Coordinates of local maxima:')\n",
    "for x, y, c in zip(x_coords, y_coords, confidences):\n",
    "    print(f'\\tAt {x},{y}, local maximum w/ confidence={c:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá llamaos a la función `get_harris_interest_points()` que está en `part1_harris_corner.py` para detectar puntos 'intersantes' en las imágenes.  \n",
    "\n",
    "**IMPORTANTE**  \n",
    "Asegúrate de agregar tu código a la función `get_harris_interest_points()` para obtener los puntos de interés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_unit_tests.test_part1_harris_corner import test_get_harris_interest_points, test_remove_border_vals\n",
    "\n",
    "print('test_remove_border_vals(): ', verify(test_remove_border_vals))\n",
    "\n",
    "print('get_harris_interest_points()', verify(test_get_harris_interest_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from proj2_code.part1_harris_corner import get_harris_interest_points\n",
    "from proj2_code.utils import show_interest_points\n",
    "\n",
    "num_interest_points = 2500\n",
    "X1, Y1, _ = get_harris_interest_points( copy.deepcopy(image1_bw), num_interest_points)\n",
    "X2, Y2, _ = get_harris_interest_points( copy.deepcopy(image2_bw), num_interest_points)\n",
    "\n",
    "num_pts_to_visualize = 300\n",
    "# Visualize the interest points\n",
    "rendered_img1 = show_interest_points(image1, X1[:num_pts_to_visualize], Y1[:num_pts_to_visualize])\n",
    "rendered_img2 = show_interest_points(image2, X2[:num_pts_to_visualize], Y2[:num_pts_to_visualize])\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1); plt.imshow(rendered_img1)\n",
    "plt.subplot(1,2,2); plt.imshow(rendered_img2)\n",
    "print(f'{len(X1)} corners in image 1, {len(X2)} corners in image 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2: Normalized Patch Feature Descriptor\n",
    "## Crear feature de vectores para cada punto de interés (Szeliski 7.1.2)\n",
    "\n",
    "Quizás la manera mas simple de describir un punto de interés es utilizar un parche de 16x16  sobre el punto de interés, convertirlo a un vector de 256 Dimensiones (con los valores de intensidad de la imagen) y luego normalizarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_code.part2_patch_descriptor import compute_normalized_patch_descriptors\n",
    "from proj2_unit_tests.test_part2_patch_descriptor import test_compute_normalized_patch_descriptors\n",
    "\n",
    "print('compute_normalized_patch_descriptors:', verify(test_compute_normalized_patch_descriptors))\n",
    "\n",
    "image1_features = compute_normalized_patch_descriptors(image1_bw, X1, Y1, feature_width=16)\n",
    "image2_features = compute_normalized_patch_descriptors(image2_bw, X2, Y2, feature_width=16)\n",
    "\n",
    "# Visualizamos como se ven los primeros 300 vectores de features para la imagen 1. (No deben ser todos iguales ni tampoco \n",
    "# imágenes negras.)\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1); plt.imshow(image1_features[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Match features (Szeliski 7.1.3, page 423)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testea tu implementacioń de feature matching\n",
    "from proj2_unit_tests.test_part3_feature_matching import (\n",
    "    test_match_features_ratio_test,\n",
    "    test_compute_feature_distances_2d,\n",
    "    test_compute_feature_distances_10d\n",
    ")\n",
    "print('compute_feature_distances (2d):', verify(test_compute_feature_distances_2d))\n",
    "print('compute_feature_distances (10d):', verify(test_compute_feature_distances_10d))\n",
    "print('match_features_ratio_test:', verify(test_match_features_ratio_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_code.part3_feature_matching import match_features_ratio_test\n",
    "\n",
    "matches, confidences = match_features_ratio_test(image1_features, image2_features)\n",
    "print('{:d} matches from {:d} corners'.format(len(matches), len(X1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Es posible que desee establecer 'num_pts_to_visualize' y 'num_pts_to_evaluate' a una constante (por ejemplo, 100) una vez que comience a detectar cientos de puntos de interés, de lo contrario, los puntos de interés se van a ver todos encimados y desordenadas. También puede establecer un umbral basado en la confianza.\n",
    "\n",
    "A continuacón, tenemos 2 funciones de visualización. Pueden comentar si quieren uno de ellos para ver mejor el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from proj2_code.utils import show_correspondence_circles, show_correspondence_lines\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "# num_pts_to_visualize = len(matches)\n",
    "num_pts_to_visualize = 200\n",
    "c1 = show_correspondence_circles(image1, image2,\n",
    "                    X1[matches[:num_pts_to_visualize, 0]], Y1[matches[:num_pts_to_visualize, 0]],\n",
    "                    X2[matches[:num_pts_to_visualize, 1]], Y2[matches[:num_pts_to_visualize, 1]])\n",
    "plt.figure(figsize=(10,5)); plt.imshow(c1)\n",
    "plt.savefig('../results/vis_circles.jpg', dpi=1000)\n",
    "c2 = show_correspondence_lines(image1, image2,\n",
    "                    X1[matches[:num_pts_to_visualize, 0]], Y1[matches[:num_pts_to_visualize, 0]],\n",
    "                    X2[matches[:num_pts_to_visualize, 1]], Y2[matches[:num_pts_to_visualize, 1]])\n",
    "plt.figure(figsize=(10,5)); plt.imshow(c2)\n",
    "plt.savefig('../results/vis_lines.jpg', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comente la función a continuación si no está probando en los pares de imágenes de Notre Dame, Episcopal Gaudi y Mount Rushmore; esta función de evaluación solo funcionará para aquellas que tengan información de validación disponible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_code.utils import evaluate_correspondence\n",
    "# num_pts_to_evaluate = len(matches)\n",
    "num_pts_to_evaluate = 2500\n",
    "_, c = evaluate_correspondence(image1, image2, eval_file, scale_factor,\n",
    "                        X1[matches[:num_pts_to_evaluate, 0]], Y1[matches[:num_pts_to_evaluate, 0]],\n",
    "                        X2[matches[:num_pts_to_evaluate, 1]], Y2[matches[:num_pts_to_evaluate, 1]])\n",
    "plt.figure(figsize=(8,4)); plt.imshow(c)\n",
    "plt.savefig('../results/eval.jpg', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 4: Sift Feature Descriptor (Szeliski 7.1.2, page 414)\n",
    "SIFT se basa en calcular las magnitudes y orientaciones de las gradientes de la imagen y luego calcular los histogramas ponderados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_unit_tests.test_part4_sift_descriptor import (\n",
    "    test_get_magnitudes_and_orientations,\n",
    "    test_get_gradient_histogram_vec_from_patch\n",
    ")\n",
    "print('get_magnitudes_and_orientations:', verify(test_get_magnitudes_and_orientations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('get_gradient_histogram_vec_from_patch():', verify(test_get_gradient_histogram_vec_from_patch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_unit_tests.test_part4_sift_descriptor import test_get_feat_vec, test_get_SIFT_descriptors\n",
    "print(verify(test_get_feat_vec))\n",
    "print(verify(test_get_SIFT_descriptors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from proj2_code.part4_sift_descriptor import get_SIFT_descriptors\n",
    "from proj2_code.utils import cheat_interest_points\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "image1_features = get_SIFT_descriptors(image1_bw, X1, Y1)\n",
    "image2_features = get_SIFT_descriptors(image2_bw, X2, Y2)\n",
    "end = time.time()\n",
    "duration = end - start\n",
    "print(f'SIFT took {duration} sec.')\n",
    "\n",
    "# visualizar cómo se ven los valores de los primeros 200 vectores de features SIFT (no deben ser idénticos o completamente negros)\n",
    "plt.figure(); plt.subplot(1,2,1); plt.imshow(image1_features[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from proj2_code.utils import show_correspondence_circles, show_correspondence_lines\n",
    "\n",
    "matches, confidences = match_features_ratio_test(image1_features, image2_features)\n",
    "print('{:d} matches from {:d} corners'.format(len(matches), len(X1)))\n",
    "\n",
    "# num_pts_to_visualize = len(matches)\n",
    "num_pts_to_visualize = 200\n",
    "c1 = show_correspondence_circles(\n",
    "    image1,\n",
    "    image2,\n",
    "    X1[matches[:num_pts_to_visualize, 0]],\n",
    "    Y1[matches[:num_pts_to_visualize, 0]],\n",
    "    X2[matches[:num_pts_to_visualize, 1]],\n",
    "    Y2[matches[:num_pts_to_visualize, 1]]\n",
    ")\n",
    "plt.figure(figsize=(10,5)); plt.imshow(c1)\n",
    "plt.savefig('../results/vis_circles.jpg', dpi=1000)\n",
    "c2 = show_correspondence_lines(\n",
    "    image1,\n",
    "    image2,\n",
    "    X1[matches[:num_pts_to_visualize, 0]],\n",
    "    Y1[matches[:num_pts_to_visualize, 0]],\n",
    "    X2[matches[:num_pts_to_visualize, 1]],\n",
    "    Y2[matches[:num_pts_to_visualize, 1]]\n",
    ")\n",
    "plt.figure(figsize=(10,5)); plt.imshow(c2)\n",
    "plt.savefig('../results/vis_lines.jpg', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_code.utils import evaluate_correspondence\n",
    "num_pts_to_evaluate = len(matches)\n",
    "_, c = evaluate_correspondence(\n",
    "    image1,\n",
    "    image2,\n",
    "    eval_file,\n",
    "    scale_factor,\n",
    "    X1[matches[:num_pts_to_evaluate, 0]],\n",
    "    Y1[matches[:num_pts_to_evaluate, 0]],\n",
    "    X2[matches[:num_pts_to_evaluate, 1]],\n",
    "    Y2[matches[:num_pts_to_evaluate, 1]]\n",
    ")\n",
    "plt.figure(figsize=(8,4)); plt.imshow(c)\n",
    "plt.savefig('../results/eval.jpg', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asegúrese de que su código se ejecuta en menos de 90 segundos y alcanza >80% de accuray en el par de Notre Dame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_unit_tests.test_part4_sift_descriptor import (\n",
    "    test_feature_matching_speed,\n",
    "    test_feature_matching_accuracy\n",
    ")\n",
    "print('SIFT pipeline speed test:', verify(test_feature_matching_speed))\n",
    "print('SIFT pipeline accuracy test:', verify(test_feature_matching_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opcional Puntaje Extra\n",
    "Implemente SIFT de una manera completamente vectorizada con al menos un 80% de precisión en Notre Dame (sin bucles sobre píxeles, como máximo un bucle sobre puntos clave). SIFT en ambas imágenes debería ejecutarse en menos de 5 segundos. La orientación en cada vector se puede obtener mediante convolución 1x1 con 8 vectores bases o de vector unitario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from proj2_code.part4_sift_descriptor import get_sift_features_vectorized\n",
    "\n",
    "start = time.time()\n",
    "image1_features = get_sift_features_vectorized(image1_bw, X1, Y1)\n",
    "image2_features = get_sift_features_vectorized(image2_bw, X2, Y2)\n",
    "end = time.time()\n",
    "duration = end - start\n",
    "print(f'SIFT took {duration} sec.')\n",
    "\n",
    "matches, confidences = match_features_ratio_test(image1_features, image2_features)\n",
    "\n",
    "num_pts_to_evaluate = len(matches) - 1\n",
    "_, c = evaluate_correspondence(\n",
    "    image1,\n",
    "    image2,\n",
    "    eval_file,\n",
    "    scale_factor,\n",
    "    X1[matches[:, 0]],\n",
    "    Y1[matches[:, 0]],\n",
    "    X2[matches[:, 1]],\n",
    "    Y2[matches[:, 1]]\n",
    ")\n",
    "plt.figure(figsize=(8,4)); plt.imshow(c)\n",
    "plt.savefig('../results/eval.jpg', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_unit_tests.test_part4_sift_descriptor import test_extra_credit_vectorized_sift\n",
    "\n",
    "print(verify(test_extra_credit_vectorized_sift))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
